{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7bdd71-c11a-49a6-b372-9ee5613f6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import google.cloud.aiplatform as aip\n",
    "import json\n",
    "from string import Template\n",
    "\n",
    "import google.auth\n",
    "from google.auth import impersonated_credentials, transport\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.v1 import bigquery as gcc_bq\n",
    "\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component, Dataset)\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "\n",
    "from kfp.v2.components import importer_node\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "from google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ef85c80-40a6-48c1-906c-f657cdfcb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def error_op(msg: str):\n",
    "    raise(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "559afd52-909e-4c38-a4a6-676759b9b920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44e57077-1655-4f4e-9356-4f506944b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"joblib\",\n",
    "        \"db-dtypes\",\n",
    "        \"pyarrow\",\n",
    "        \"pandas-gbq\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"pytz\"\n",
    "    ],\n",
    ")\n",
    "def prediction(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "):  \n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    import pandas_gbq\n",
    "    from google.cloud import storage\n",
    "    from joblib import load\n",
    "    from io import BytesIO\n",
    "    from pytz import timezone\n",
    "    \n",
    "    tz = timezone('America/Lima')\n",
    "    FORMAT_DATE = \"%Y-%m-%d\"\n",
    "    \n",
    "    # Cliente BigQuery\n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    # Leer datos de BigQuery\n",
    "    X_train = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=source_x_train_table)).to_dataframe()\n",
    "\n",
    "    \n",
    "    # Leer características seleccionadas de BigQuery\n",
    "    features = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=features_table)).to_dataframe()\n",
    "    \n",
    "    features = features[\"string_field_0\"].tolist()\n",
    "    \n",
    "\n",
    "    X_train = X_train[features]\n",
    "    \n",
    "    def generate_datetime_created():\n",
    "        return datetime.now()\n",
    "    \n",
    "    def generate_date_created():\n",
    "        return datetime.now(tz).date().strftime(FORMAT_DATE)\n",
    "    \n",
    "    \n",
    "    def load_model_from_gcs(path_model):\n",
    "        # Inicializar el cliente de Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Obtener el nombre del bucket y la ruta del objeto\n",
    "        bucket_name, blob_name = path_model.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "\n",
    "        # Obtener el objeto desde Cloud Storage\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        model_bytes = blob.download_as_string()\n",
    "\n",
    "        # Cargar el modelo desde los bytes obtenidos\n",
    "        classifier = load(BytesIO(model_bytes))\n",
    "\n",
    "        return classifier\n",
    "\n",
    "    classifier = load_model_from_gcs(path_model)\n",
    "\n",
    "    # Realizar la predicción\n",
    "    predictions = classifier.predict(X_train)\n",
    "    predictions = pd.DataFrame(predictions, columns=['prediction'])\n",
    "\n",
    "    # Obtener el user_id de la sesión actual en BigQuery\n",
    "    user_id = client.query(\"SELECT SESSION_USER()\").to_dataframe().iloc[0, 0]\n",
    "\n",
    "    # Agregar campos de auditoría\n",
    "    start_time = generate_datetime_created()\n",
    "    execute_date = generate_date_created()\n",
    "    \n",
    "    predictions['creation_user'] = user_id\n",
    "    predictions['process_date'] = datetime.strptime(execute_date, '%Y-%m-%d')\n",
    "    predictions['process_date'] = pd.to_datetime(predictions['process_date']).dt.date\n",
    "    predictions['load_date'] = pd.to_datetime(start_time)\n",
    "    \n",
    "    # Guardar el resultado en BigQuery \n",
    "    pandas_gbq.to_gbq(predictions , table_id, if_exists='append', project_id=project)\n",
    "\n",
    "    print(\"Predicción generada y guardada en BigQuery.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d6605f-8ca6-4287-bc46-7ad62a8cabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"intro\", \n",
    "    description=\"intro\",\n",
    "    pipeline_root=\"gs://<bucket>/demo\"\n",
    ")\n",
    "\n",
    "def main_pipeline(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "    \n",
    "    notify_email_task = VertexNotificationEmailOp(recipients=[\"laybarm@intercorp.com.pe\"])\n",
    "    notify_email_task.set_display_name('Notification Email')\n",
    "    \n",
    "    with dsl.ExitHandler(notify_email_task, name=\"Execute pipeline prediction\"):\n",
    "\n",
    "        validate_tables_job = validate_data(\n",
    "            source_x_train_table = source_x_train_table\n",
    "        )\n",
    "        validate_tables_job.set_display_name('Validate Data')\n",
    "\n",
    "        with dsl.Condition(\n",
    "            validate_tables_job.outputs['condition']==\"false\",\n",
    "            name=\"no-execute\",\n",
    "        ):\n",
    "            error_op(\"No se logro validar las tablas de ingesta.\")\n",
    "\n",
    "\n",
    "        with dsl.Condition(\n",
    "            validate_tables_job.outputs['condition']==\"true\",\n",
    "            name=\"execute\",\n",
    "        ):\n",
    "  \n",
    "            prediction_tast = prediction(\n",
    "                project = project,\n",
    "                source_x_train_table = source_x_train_table,\n",
    "                features_table = features_table,\n",
    "                table_id = table_id,\n",
    "                path_model = path_model,\n",
    "            )\n",
    "            prediction_tast.set_display_name(\"PREDICTION_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09581f-2348-45b5-8533-225e3d04d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler as v2_compiler\n",
    "\n",
    "v2_compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path=\"pipeline_prediction.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4e0a1-9ca9-4a10-964d-a2a9469af54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"Archivo {source_file_name} subido a {destination_blob_name} en el bucket {bucket_name}.\")\n",
    "\n",
    "# Define las variables\n",
    "bucket_name = \"<bucket>\"\n",
    "destination_blob_name = \"demo/pipeline_prediction.json\"\n",
    "pipeline_file = \"pipeline_prediction.json\"\n",
    "# Llamar a la función para subir el archivo\n",
    "upload_to_gcs(bucket_name, pipeline_file, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d30531d7-f798-420e-9cde-d8bd781deb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=\"<project_id>\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e7da3-41a5-4a91-b763-a36acfdc6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline de prueba\",\n",
    "    template_path=\"gs://<bucket>/demo/pipeline_prediction.json\",\n",
    "    #job_id=\"mlops72\",\n",
    "    enable_caching=False,\n",
    "    project=\"<project_id>\",\n",
    "    location=\"us-central1\",\n",
    "    parameter_values={\"project\": \"<project_id>\", \n",
    "                      \"source_x_train_table\": \"<project_id>.<dataset>.xtrain\",\n",
    "                      \"features_table\": \"<project_id>.<dataset>.selected_features\",\n",
    "                      \"table_id\": \"<project_id>.<dataset>.predictions\",\n",
    "                      \"path_model\": \"gs://<bucket>/demo/data/model/model.joblib\"\n",
    "                     },\n",
    "    failure_policy = 'slow'\n",
    "    #labels={\"module\": \"ml\", \"application\": \"app\", \"chapter\": \"mlops\", \"company\": \"datapat\", \"environment\": \"dev\", \"owner\": \"xxxx\"}\n",
    ")\n",
    "\n",
    "print('submit pipeline job ...')\n",
    "job.submit(service_account=\"dev-dp-ml-vertex@<project_id>.iam.gserviceaccount.com\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
